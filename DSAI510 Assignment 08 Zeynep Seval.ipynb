{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413ac86d",
   "metadata": {},
   "source": [
    "## Assignment 8 - Deadline: Dec 29, 2024, Sun 11pm\n",
    "\n",
    "#### DSAI 510 Fall 2024\n",
    "\n",
    "Complete the assignment below and upload <span style=\"color:red\">both the .ipynb file and its pdf</span> to https://moodle.boun.edu.tr by the deadline given above. The submission page on Moodle will close automatically after this date and time.\n",
    "\n",
    "\n",
    "To make a pdf, this may work: Hit CMD+P or CTRL+P, and save it as PDF. You may also use other options from the File menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2c49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set the display option to show all rows scrolling with a slider\n",
    "pd.set_option('display.max_rows', None)\n",
    "# To disable this, run the line below:\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c6af7",
   "metadata": {},
   "source": [
    "## Note: \n",
    "In the problems below, if they ask \"show the number of records that are nonzero\", \n",
    "the answer is a number; so you don't need to show the records themselves.\n",
    "But if it asks, \"show the records with NaN\", it wants you to print those records (rows)\n",
    "containing NAN and other entries, not asking how many such records there are. So be careful about what you're asked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8fdff",
   "metadata": {},
   "source": [
    "# Problem 1: Newsgroup discussions classification (30 pts)\n",
    "\n",
    "The `fetch_20newsgroups` dataset is a collection of approximately 20,000 newsgroup documents, distributed across 20 different newsgroups. Each newsgroup represents a distinct category, ranging from technology and politics to religion and sports. This dataset is widely used in natural language processing and machine learning for tasks like text classification and clustering, due to its diverse range of topics and real-world discussion content.\n",
    "\n",
    "Your goal is to build a classifier that would take a document from this dataset and output the category of that document. Categories: 'sci.med', 'comp.graphics', 'sci.electronics' and 'talk.politics.misc'.\n",
    "\n",
    "The data is already loaded below and first five documents and their class labels from the dataset are printed.\n",
    "\n",
    "1) Use TF-IDF to vectorize the train and test dataset. \n",
    "\n",
    "(Important to prevent data leakage:\n",
    "Remember from the the ipynb notebook we used in the lecture; you use `fit_transform()` on the train dataset to get the vocabulary (list of words), and for the test dataset, you only use `transform()` to learn the vocabulary from the test dataset to represent the test documents in terms of vocabulary learned from the train data. It is likely that there will be words that only appear in the test dataset, but does not occur in the train dataset. Those words will simply be ignored when the model is used on the test dataset to extract the frequency table, because vocabulary is set by the train dataset when we use `fit_transform()` on the train dataset.)\n",
    "\n",
    "2) Use a classifier of your choice to train your model and calculate the accuracy of your model by using the test data.\n",
    "\n",
    "(Note that we have four classes here, so your classifier will not be a binary classifier but a multi-class classifier. This means your model should be capable of distinguishing and predicting among four different categories.)\n",
    "\n",
    "\n",
    "3) Find one document from the test data for each categories: 'sci.med', 'comp.graphics', 'sci.electronics' and 'talk.politics.misc'. Print those four documents and print their category as a string (not as a number, but as 'sci.med' etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957149c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Selected categories \n",
    "categories = ['sci.med', 'comp.graphics', 'sci.electronics', 'talk.politics.misc']\n",
    "\n",
    "# Loading training dataset\n",
    "df_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=3)\n",
    "\n",
    "# Loading testing dataset\n",
    "df_test = fetch_20newsgroups(subset='test',categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1875a14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c5facc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0d85a",
   "metadata": {},
   "source": [
    "### First four documents as examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4446ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mDocument 0:\u001b[0m \n",
      "From: peter.m@insane.apana.org.au (Peter Tryndoch)\n",
      "Subject: Dmm Advice Needed\n",
      "Lines: 28\n",
      "\n",
      "AllMartin EmdeDMM Advice Needed\n",
      "\n",
      "ME>From: mce5921@bcstec.ca.boeing.com (Martin Emde)\n",
      "ME>Organization: Boeing\n",
      "ME> \n",
      "ME>I an currely in the market for a DMM and recently saw an add\n",
      "ME>for a Kelvin 94 ($199).  Does anyone own one of these or some\n",
      "ME>other brand that they are extremely happy with.  How do the \n",
      "ME>small name brands compare with the Fluke and Beckman brands?\n",
      "ME>I am willing to spend ~$200 for one.\n",
      "ME> \n",
      "ME>Any help is greatly appreciated. (please email)\n",
      "ME> \n",
      "ME>-Martin\n",
      "\n",
      "If you are going to use one where it counts (eg:aviation, space scuttle, \n",
      "etc) then I suggest you go and buy a Fluke (never seen a Beckman), however \n",
      "for every other use you can buy a cheapie. I have a metex which is some \n",
      "made up name, as I have seen the same DMM with other brand names on it, I \n",
      "bought it about 4 yrs ago for Aus$125.00 (convert that to US and you see \n",
      "that it's definetly a cheapie.) So far it has proved to be accurate, taken \n",
      "moderate abuse, and has many features on it (CAP, FREQ,Transistor check, \n",
      "etc). I am very happy with it and would definetly not buy a fluke just for \n",
      "the name. Hope this helps.\n",
      "\n",
      "Cheers \n",
      "Peter T.\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\u001b[1m\u001b[91mDocument 1:\u001b[0m \n",
      "From: bmdelane@quads.uchicago.edu (brian manning delaney)\n",
      "Subject: Re: diet for Crohn's (IBD)\n",
      "Reply-To: bmdelane@midway.uchicago.edu\n",
      "Organization: University of Chicago\n",
      "Lines: 27\n",
      "\n",
      "One thing that I haven't seen in this thread is a discussion of the\n",
      "relation between IBD inflammation and the profile of ingested fatty\n",
      "acids (FAs).\n",
      "\n",
      "I was diagnosed last May w/Crohn's of the terminal ileum. When I got\n",
      "out of the hospital I read up on it a bit, and came across several\n",
      "studies investigating the role of EPA (an essentially FA) in reducing\n",
      "inflammation. The evidence was mixed. [Many of these studies are\n",
      "discussed in \"Inflammatory Bowel Disease,\" MacDermott, Stenson. 1992.]\n",
      "\n",
      "But if I recall correctly, there were some methodological bones to be\n",
      "picked with the studies (both the ones w/pos. and w/neg. results). In\n",
      "the studies patients were given EPA (a few grams/day for most of the\n",
      "studies), but, if I recall correctly, there was no restriction of the\n",
      "_other_ FAs that the patients could consume. From the informed\n",
      "layperson's perspective, this seems mistaken. If lots of n-6 FAs are\n",
      "consumed along with the EPA, then the ratio of \"bad\" prostanoid\n",
      "products to \"good\" prostanoid products could still be fairly \"bad.\"\n",
      "Isn't this ratio the issue?\n",
      "\n",
      "What's the view of the gastro. community on EPA these days? EPA\n",
      "supplements, along with a fairly severe restriction of other FAs\n",
      "appear to have helped me significantly (though it could just be the\n",
      "low absolute amount of fat I eat -- 8-10% calories).\n",
      "\n",
      "-Brian <bmdelane@midway.uchicago.edu>\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\u001b[1m\u001b[91mDocument 2:\u001b[0m \n",
      "From: johnh@macadam.mpce.mq.edu.au (John Haddy)\n",
      "Subject: Re: Help with ultra-long timing\n",
      "Organization: Macquarie University\n",
      "Lines: 39\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: macadam.mpce.mq.edu.au\n",
      "\n",
      "In article <C513wI.G5A@athena.cs.uga.edu>, mcovingt@aisun3.ai.uga.edu (Michael Covington) writes:\n",
      "|> (1) Don't use big capacitors.  They are unreliable for timing due to\n",
      "|> leakage. \n",
      "|> \n",
      "|> Instead, use a quartz crystal and divide its frequency by 2 40 times\n",
      "|> or something like that.\n",
      "|> \n",
      "|> 1 MHz divided by 2^40 = 1 cycle per 2 weeks, approximately.\n",
      "|> \n",
      "|> (2) I wouldn't expect any components (other than batteries or electrolytic\n",
      "|> capacitors) to fail at -40 C (or -40 F for that matter either :) ).\n",
      "|> The battery is going to be your big problem.  Also, of course, your\n",
      "|> circuit shouldn't depend on exact values of resistors (which a crystal-\n",
      "|> controlled timer won't).\n",
      "|> \n",
      "\n",
      "... Wouldn't a crystal be affected by cold? My gut feeling is that, as a\n",
      "mechanically resonating device, extreme cold is likely to affect the\n",
      "compliance (?terminology?) of the quartz, and hence its resonant frequency.\n",
      "\n",
      "|> -- \n",
      "|> :-  Michael A. Covington         internet mcovingt@ai.uga.edu :    *****\n",
      "|> :-  Artificial Intelligence Programs       phone 706 542-0358 :  *********\n",
      "|> :-  The University of Georgia                fax 706 542-0349 :   *  *  *\n",
      "|> :-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **\n",
      "\n",
      "\n",
      "JohnH\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "      |  _  |_   _   |_|  _   _|  _|              Electronics Department\n",
      "    |_| (_) | | | |  | | (_| (_| (_| \\/           School of MPCE\n",
      "    ---------------------------------/-           Macquarie University\n",
      "                                                  Sydney, AUSTRALIA 2109\n",
      "\n",
      "    Email: johnh@mpce.mq.edu.au, Ph: +61 2 805 8959, Fax: +61 2 805 8983\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\u001b[1m\u001b[91mDocument 3:\u001b[0m \n",
      "From: bob1@cos.com (Bob Blackshaw)\n",
      "Subject: Re: Tieing Abortion to Health Reform -- Is Clinton Nuts?\n",
      "Organization: Corporation for Open Systems\n",
      "Distribution: world \n",
      "Lines: 37\n",
      "\n",
      "In <1993Apr5.170349.10700@ringer.cs.utsa.edu> sbooth@lonestar.utsa.edu (Simon E. Booth) writes:\n",
      "\n",
      ">In article <1993Apr2.230831.18332@wdl.loral.com> bard@cutter.ssd.loral.com writes:\n",
      ">>sbooth@lonestar.utsa.edu (Simon E. Booth) writes:\n",
      ">># sandvik@newton.apple.com (Kent Sandvik) writes:\n",
      ">># >We already kill people (death penalty), and that costs even more\n",
      ">># >money, so you could as well complain about this extremely barbaric\n",
      ">># >way of justice.\n",
      ">>#\n",
      ">># But the death penalty is right.\n",
      ">>#\n",
      ">># And how expensive can an execution be? I mean, I think rope, cyanide\n",
      ">># (for the gas), or the rifles and ammunition to arm firing squads are\n",
      ">># affordable.\n",
      ">>#\n",
      ">># Now, perhaps lethal injection might be expensive, in that case, let's\n",
      ">># return to the more efficient methods employed in the past.\n",
      ">>\n",
      ">>Oh, sure, the death *penalty* is fairly inexpensive, but the trial and\n",
      ">>sentencing can run millions.\n",
      "\n",
      ">>\n",
      ">>--strychnine\tunless you wanna cut costs by skipping the trial and\n",
      ">> sentencing... you murderous little rat-bastard\n",
      "\n",
      ">  Why as a matter of fact, I was thinking of that as a way to make\n",
      ">the system more efficient.  And the only murderous rat-bastards are\n",
      ">aboritionists.\n",
      "\n",
      "Yeah, Simon's no rat-bastard, he's the Head Attack Puppy :-)\n",
      "\n",
      "\n",
      ">Simon\n",
      "\n",
      "\n",
      "TOG\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\u001b[1m\u001b[91mDocument 4:\u001b[0m \n",
      "From: rscharfy@magnus.acs.ohio-state.edu (Ryan C Scharfy)\n",
      "Subject: Re: Good Neighbor Political Hypocrisy Test\n",
      "Nntp-Posting-Host: magnusug.magnus.acs.ohio-state.edu\n",
      "Organization: The Ohio State University\n",
      "Lines: 59\n",
      "\n",
      "In article <1993Apr16.141409.25036@pmafire.inel.gov> cdm@pmafire.inel.gov (Dale\n",
      " Cook) writes:\n",
      ">In article <1993Apr15.193603.14228@magnus.acs.ohio-state.edu> rscharfy@magnus.\n",
      "acs.ohio-state.edu (Ryan C Scharfy) writes:\n",
      ">>In article <stevethC5JGCr.1Ht@netcom.com> steveth@netcom.com (Steve Thomas) w\n",
      "ri\n",
      ">>tes:\n",
      ">>\n",
      ">>>Just _TRY_ to justify the War On Drugs, I _DARE_ you!\n",
      ">>\n",
      ">>A friend of mine who smoke pot every day and last Tuesday took 5 hits of acid\n",
      "\n",
      ">>is still having trouble \"aiming\" for the bowl when he takes a dump.  Don't as\n",
      "\n",
      ">>me how, I just have seen the results.\n",
      ">>\n",
      ">>Boy, I really wish we we cut the drug war and have more people screwed up in\n",
      ">>the head.\n",
      ">\n",
      ">I'm sorry about your friend.  Really.  But this anecdote does nothing to\n",
      ">justify the \"war on drugs\".  If anything, it demonstrates that the \"war\"\n",
      ">is a miserable failure.  What it demonstrates is that people will take\n",
      ">drugs if they want to, legal or not.  Perhaps if your friend were taking\n",
      ">legal, regulated drugs under a doctors supervision he might not be in the\n",
      ">position he's in now.\n",
      ">\n",
      "\n",
      "I do agree with you, in a way.  The war on drugs has failed, but in my opinion,\n",
      "that doesn't mean we have to give up.  Only change the tactics.\n",
      "\n",
      "For instance, here are how some penalties should be changed.\n",
      "\n",
      "Dealing Coke -- Death\n",
      "Dealing Heroin -- Death\n",
      "Dealing Pot -- Death\n",
      "Dealing Crack -- Death\n",
      "\n",
      "The list goes on and on!!!......\n",
      "\n",
      "JUST KIDDING!!!\n",
      "\n",
      "However, on a more serious note, I do believe that we should take some money \n",
      "away from the foriegn operations in South America and costly border \n",
      "interdiction efforts.  (Don't think I'm going to say, \"spend it to educate \n",
      "people\", because I know plenty of educated dopers).  Actually, spend it on  \n",
      "things like drug treatment programs.\n",
      "\n",
      "I saw an interesting story on 60 minutes about how the British actually \n",
      "prescribe and addict his \"recommended\" dosage, and try to ween him off from it,\n",
      "or cut the amount down to levels where it is \"acceptable\".  Sounds good so far \n",
      "from what I heard with a decrease in cost, lower addiction rates by wiping out \n",
      "the dealer's markets, etc. (But that was the only thing I have heard about it.)\n",
      "\n",
      "However, legalizing it and just sticking some drugs in gas stations to be \n",
      "bought like cigarettes is just plain silly.  Plus, I have never heard of a \n",
      "recommended dosage for drugs like crack, ecstasy, chrystal meth and LSD.\n",
      "The 60 Minute Report said it worked with \"cocaine\" cigarettes, pot and heroin.\n",
      "\n",
      "Ryan\n",
      "\n",
      "\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"\\033[1m\\033[91mDocument {i}:\\033[0m \")  # Red and bold text\n",
    "    print(df_train.data[i])\n",
    "    print(\"\\n------------------\\n\")  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da9702f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\seval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# Create lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize and filter short tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e76606f-8dd3-4ac7-a767-b86cd8fe7c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: TF-IDF Vectorization\n",
      "Number of classes: 4\n",
      "Unique classes in training data: 4\n",
      "\n",
      "Step 2: Training Classifier\n",
      "Shape of probability predictions: (1488, 4)\n",
      "\n",
      "Model Accuracy: 0.9147\n",
      "\n",
      "Detailed Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           sci.med       0.89      0.93      0.91       389\n",
      "     comp.graphics       0.89      0.89      0.89       393\n",
      "   sci.electronics       0.94      0.91      0.93       396\n",
      "talk.politics.misc       0.94      0.93      0.94       310\n",
      "\n",
      "          accuracy                           0.91      1488\n",
      "         macro avg       0.92      0.92      0.92      1488\n",
      "      weighted avg       0.92      0.91      0.91      1488\n",
      "\n",
      "\n",
      "Step 3: Example Documents from Test Set:\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: sci.med\n",
      "Predicted Category: sci.med\n",
      "Document:\n",
      "From: lee@hobbes.cs.umass.edu (Peter Lee)\n",
      "Subject: Re: QuickTime performance (was Re: Rumours about 3DO ???)\n",
      "\t<1993Apr16.212441.34125@rchland.ibm.com>\n",
      "\t<1993Apr26.170915.15833@waikato.ac.nz>\n",
      "Reply-To: lee@cs.umass.edu\n",
      "Organization: Software Development Lab, UMass, Amherst\n",
      "Lines: 108\n",
      "In-reply-to: ldo...\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: comp.graphics\n",
      "Predicted Category: comp.graphics\n",
      "Document:\n",
      "From: schuch@phx.mcd.mot.com (John Schuch)\n",
      "Subject: Re: Self-destructing copy protection on VHS tape?\n",
      "Nntp-Posting-Host: bopper2.phx.mcd.mot.com\n",
      "Organization: Motorola Computer Group, Tempe, Az.\n",
      "Lines: 18\n",
      "\n",
      "In article <klp.735603389@quark> klp@doe.carleton.ca (Ka Lun Pang) writes:\n",
      ">\n",
      ">I borrowed a VHS...\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: sci.electronics\n",
      "Predicted Category: sci.electronics\n",
      "Document:\n",
      "Organization: Arizona State University\n",
      "From: <ICBAL@ASUACAD.BITNET>\n",
      "Subject: Re: Opinions on Allergy (Hay Fever) shots?\n",
      "Distribution: world\n",
      " <93115.120409ICBAL@ASUACAD.BITNET> <1rhb0e$9ks@europa.eng.gtefsd.com>\n",
      "Lines: 13\n",
      "\n",
      "In article <1rhb0e$9ks@europa.eng.gtefsd.com>, draper@gnd1.wtp.gtefsd.com (PAM...\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: talk.politics.misc\n",
      "Predicted Category: sci.electronics\n",
      "Document:\n",
      "From: lazlo@carina.unm.edu (Lazlo Nibble)\n",
      "Subject: Re: WACO burning\n",
      "Organization: Vroom Socko International Fear Club\n",
      "Lines: 11\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: carina.unm.edu\n",
      "\n",
      "russotto@eng.umd.edu (Matthew T. Russotto) writes:\n",
      "\n",
      "> The idea that kerosene lamps would be all over the place (with\n",
      "...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "# TF-IDF Vectorization standard one\n",
    "print(\"Step 1: TF-IDF Vectorization\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Use built-in English stop words\n",
    "    token_pattern=r'\\b[a-zA-Z]{2,}\\b',  # Only take words with 2 or more characters\n",
    "    lowercase=True  # Convert all text to lowercase\n",
    ")\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train.data)\n",
    "\n",
    "# Only transform test data using vocabulary from training data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test.data)\n",
    "\n",
    "# Verify this is multiclass\n",
    "print(f\"Number of classes: {len(categories)}\")\n",
    "print(f\"Unique classes in training data: {len(set(df_train.target))}\")\n",
    "\n",
    "# Part 2\n",
    "# Train classifier using OneVsRestClassifier explicitly for multiclass\n",
    "print(\"\\nStep 2: Training Classifier\")\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "clf.fit(X_train_tfidf, df_train.target)\n",
    "\n",
    "# Get probability estimates for each class\n",
    "probabilities = clf.predict_proba(X_test_tfidf)\n",
    "print(f\"Shape of probability predictions: {probabilities.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(df_test.target, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(df_test.target, y_pred, target_names=categories))\n",
    "\n",
    "# Part 3\n",
    "# Find and print one document from each category\n",
    "print(\"\\nStep 3: Example Documents from Test Set:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for category_idx, category_name in enumerate(categories):\n",
    "    # Find first document in test set for this category\n",
    "    doc_idx = next(i for i, label in enumerate(df_test.target) if label == category_idx)\n",
    "    \n",
    "    # Get the predicted category for this document\n",
    "    pred_category = categories[y_pred[doc_idx]]\n",
    "    \n",
    "    print(f\"\\nCategory: {category_name}\")\n",
    "    print(f\"Predicted Category: {pred_category}\")\n",
    "    print(\"Document:\")\n",
    "    # Print first 300 characters for readability\n",
    "    print(df_test.data[doc_idx][:300] + \"...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c913c56-c683-4744-9fda-13336b59130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: TF-IDF Vectorization\n",
      "Number of classes: 4\n",
      "Unique classes in training data: 4\n",
      "\n",
      "Step 2: Training Classifier\n",
      "Shape of probability predictions: (1488, 4)\n",
      "\n",
      "Model Accuracy: 0.9254\n",
      "\n",
      "Detailed Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           sci.med       0.94      0.89      0.92       389\n",
      "     comp.graphics       0.91      0.93      0.92       393\n",
      "   sci.electronics       0.91      0.95      0.93       396\n",
      "talk.politics.misc       0.95      0.93      0.94       310\n",
      "\n",
      "          accuracy                           0.93      1488\n",
      "         macro avg       0.93      0.93      0.93      1488\n",
      "      weighted avg       0.93      0.93      0.93      1488\n",
      "\n",
      "\n",
      "Step 3: Example Documents from Test Set:\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: sci.med\n",
      "Predicted Category: sci.med\n",
      "Document:\n",
      "From: lee@hobbes.cs.umass.edu (Peter Lee)\n",
      "Subject: Re: QuickTime performance (was Re: Rumours about 3DO ???)\n",
      "\t<1993Apr16.212441.34125@rchland.ibm.com>\n",
      "\t<1993Apr26.170915.15833@waikato.ac.nz>\n",
      "Reply-To: lee@cs.umass.edu\n",
      "Organization: Software Development Lab, UMass, Amherst\n",
      "Lines: 108\n",
      "In-reply-to: ldo...\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: comp.graphics\n",
      "Predicted Category: comp.graphics\n",
      "Document:\n",
      "From: schuch@phx.mcd.mot.com (John Schuch)\n",
      "Subject: Re: Self-destructing copy protection on VHS tape?\n",
      "Nntp-Posting-Host: bopper2.phx.mcd.mot.com\n",
      "Organization: Motorola Computer Group, Tempe, Az.\n",
      "Lines: 18\n",
      "\n",
      "In article <klp.735603389@quark> klp@doe.carleton.ca (Ka Lun Pang) writes:\n",
      ">\n",
      ">I borrowed a VHS...\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: sci.electronics\n",
      "Predicted Category: sci.electronics\n",
      "Document:\n",
      "Organization: Arizona State University\n",
      "From: <ICBAL@ASUACAD.BITNET>\n",
      "Subject: Re: Opinions on Allergy (Hay Fever) shots?\n",
      "Distribution: world\n",
      " <93115.120409ICBAL@ASUACAD.BITNET> <1rhb0e$9ks@europa.eng.gtefsd.com>\n",
      "Lines: 13\n",
      "\n",
      "In article <1rhb0e$9ks@europa.eng.gtefsd.com>, draper@gnd1.wtp.gtefsd.com (PAM...\n",
      "--------------------------------------------------\n",
      "\n",
      "Category: talk.politics.misc\n",
      "Predicted Category: comp.graphics\n",
      "Document:\n",
      "From: lazlo@carina.unm.edu (Lazlo Nibble)\n",
      "Subject: Re: WACO burning\n",
      "Organization: Vroom Socko International Fear Club\n",
      "Lines: 11\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: carina.unm.edu\n",
      "\n",
      "russotto@eng.umd.edu (Matthew T. Russotto) writes:\n",
      "\n",
      "> The idea that kerosene lamps would be all over the place (with\n",
      "...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Trial with Lemmatization to increase accuracy\n",
    "# Part 1\n",
    "# TF-IDF Vectorization with custom preprocessor function including Lemmatization\n",
    "print(\"Step 1: TF-IDF Vectorization\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    preprocessor=preprocess_text,\n",
    "    stop_words=None,  # We'll handle stop words in preprocessing\n",
    "    token_pattern=r'\\b[a-zA-Z]{2,}\\b'  # Only take words with 2 or more characters\n",
    ")\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train.data)\n",
    "\n",
    "# Only transform test data using vocabulary from training data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test.data)\n",
    "\n",
    "# Verify this is multiclass\n",
    "print(f\"Number of classes: {len(categories)}\")\n",
    "print(f\"Unique classes in training data: {len(set(df_train.target))}\")\n",
    "\n",
    "# Part 2\n",
    "# Train classifier using OneVsRestClassifier explicitly for multiclass\n",
    "print(\"\\nStep 2: Training Classifier\")\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "clf.fit(X_train_tfidf, df_train.target)\n",
    "\n",
    "# Get probability estimates for each class\n",
    "probabilities = clf.predict_proba(X_test_tfidf)\n",
    "print(f\"Shape of probability predictions: {probabilities.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(df_test.target, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(df_test.target, y_pred, target_names=categories))\n",
    "\n",
    "# Part 3\n",
    "# Find and print one document from each category\n",
    "print(\"\\nStep 3: Example Documents from Test Set:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for category_idx, category_name in enumerate(categories):\n",
    "    # Find first document in test set for this category\n",
    "    doc_idx = next(i for i, label in enumerate(df_test.target) if label == category_idx)\n",
    "    \n",
    "    # Get the predicted category for this document\n",
    "    pred_category = categories[y_pred[doc_idx]]\n",
    "    \n",
    "    print(f\"\\nCategory: {category_name}\")\n",
    "    print(f\"Predicted Category: {pred_category}\")\n",
    "    print(\"Document:\")\n",
    "    # Print first 300 characters for readability\n",
    "    print(df_test.data[doc_idx][:300] + \"...\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2020ab-ac4e-4151-bf6c-e0e452d02994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilities for misclassified document:\n",
      "sci.med: 0.0674\n",
      "comp.graphics: 0.3968\n",
      "sci.electronics: 0.3505\n",
      "talk.politics.misc: 0.1853\n",
      "\n",
      "Top 10 most important words in this document:\n",
      "lazlo: 0.4226\n",
      "idea: 0.2585\n",
      "sense: 0.2232\n",
      "russottoengumdedu: 0.2113\n",
      "russotto: 0.2113\n",
      "spill: 0.2006\n",
      "ignite: 0.1930\n",
      "carinaunmedu: 0.1872\n",
      "tank: 0.1824\n",
      "hay: 0.1783\n",
      "\n",
      "Full document content:\n",
      "From: lazlo@carina.unm.edu (Lazlo Nibble)\n",
      "Subject: Re: WACO burning\n",
      "Organization: Vroom Socko International Fear Club\n",
      "Lines: 11\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: carina.unm.edu\n",
      "\n",
      "russotto@eng.umd.edu (Matthew T. Russotto) writes:\n",
      "\n",
      "> The idea that kerosene lamps would be all over the place (with\n",
      "> electricity cut off) makes sense.  The idea that ramming tanks into the\n",
      "> building would spill them and cause a fire makes sense.\n",
      "\n",
      "As does the idea that a CS gas canister can get hot enough to ignite dry\n",
      "baled hay.\n",
      "\n",
      "--\n",
      "Lazlo (lazlo@triton.unm.edu)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So with lemmatization accuracy increased from 91% to 93%, which is ok\n",
    "\"\"\"\n",
    "The performance metrics for each class are quite balanced:\n",
    "sci.med: precision=0.94, recall=0.89\n",
    "comp.graphics: precision=0.91, recall=0.93\n",
    "sci.electronics: precision=0.91, recall=0.95\n",
    "talk.politics.misc: precision=0.95, recall=0.93\n",
    "\n",
    "The last document (talk.politics.misc) was misclassified as comp.graphics\n",
    "This is actually an interesting case to examine since it's one of the 7% of cases where our model made a mistake\n",
    "\"\"\"\n",
    "# Let's see the actual probabilities for this misclassified document\n",
    "# Find the index of our misclassified document\n",
    "misclassified_idx = next(i for i, (true, pred) in enumerate(zip(df_test.target, y_pred)) \n",
    "                        if true == 3 and pred != 3)  # 3 is the index for talk.politics.misc\n",
    "\n",
    "# Get probabilities for this document\n",
    "doc_probs = probabilities[misclassified_idx]\n",
    "print(\"\\nProbabilities for misclassified document:\")\n",
    "for category, prob in zip(categories, doc_probs):\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# 2. Let's look at the most important features (words) for this document\n",
    "# Get the feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the TF-IDF scores for this document\n",
    "doc_vector = X_test_tfidf[misclassified_idx]\n",
    "\n",
    "# Create a list of (word, tfidf_score) pairs\n",
    "word_scores = [(feature_names[i], doc_vector[0, i]) \n",
    "               for i in doc_vector.nonzero()[1]]\n",
    "\n",
    "# Sort by TF-IDF score\n",
    "word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 most important words in this document:\")\n",
    "for word, score in word_scores[:10]:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "# 3. Print the full content of this document\n",
    "print(\"\\nFull document content:\")\n",
    "print(df_test.data[misclassified_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1440fc-f2de-42a7-b5f6-e3e12600ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model got confused due to Names, emails\n",
    "Here we can Create separate features for domains and organizations\n",
    "or Use a more sophisticated model that can weight these features appropriately\n",
    "or Add domain-based feature engineering\n",
    "...but this is not required for this assignment, so we keep 93% accuracy as it:)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
